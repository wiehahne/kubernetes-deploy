kubectl apply -nprod -f 1_initial_architecture_deployment_travelapp.yaml
kubectl apply -nprod -f 1_initial_architecture_services_travelapp.yaml

Get travel app pod name:
kubectl get pods -nprod -l app=travel

Exec to travel app:
kubectl exec -nprod -it demoapp-frontend-688d85cff-78rxk bash

Curl Summer / Winter apps:
curl demoapp-app1-summer.svc.cluster.local:9080;echo
curl winter-v1.prod.svc.cluster.local:9080;echo

!!!
PATCH PODS FORCING RECREATION:
kubectl patch deployment dj -nprod -p "{\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"date\":\"`date +'%s'`\"}}}}}"
kubectl patch deployment metal-v1 -nprod -p "{\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"date\":\"`date +'%s'`\"}}}}}"
kubectl patch deployment jazz-v1 -nprod -p "{\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"date\":\"`date +'%s'`\"}}}}}"
!!!



---
Working notes:

kubectl apply -f mesh-definition.yaml
kubectl apply -f virtual-node-definition.yaml
kubectl apply -f virtual-service-definition.yaml
kubectl apply -f controller-deployment.yaml
kubectl create -f nodes_representing_virtual_services.yaml


kubectl create -nprod -f nodes_representing_physical_services.yaml

kubectl apply -nprod -f virtual-services.yaml

Service Cluster IPs:
Admin:~/environment/travelapp/MB3/4_EKS App Create Initial Mesh Components (master) $ kubectl get svc -nprod
NAME        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
summer-v1   ClusterIP   10.100.169.2     <none>        9080/TCP   47m
travel      ClusterIP   10.100.115.232   <none>        9080/TCP   47m
winter-v1   ClusterIP   10.100.217.51    <none>        9080/TCP   47m

Admin:~/environment/travelapp/MB3/4_EKS App Create Initial Mesh Components (master) $ kubectl get svc -nprod
NAME        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
summer      ClusterIP   10.100.49.207    <none>        9080/TCP   19s
summer-v1   ClusterIP   10.100.169.2     <none>        9080/TCP   48m
travel      ClusterIP   10.100.115.232   <none>        9080/TCP   48m
winter      ClusterIP   10.100.143.136   <none>        9080/TCP   19s
winter-v1   ClusterIP   10.100.217.51    <none>        9080/TCP   48m



kubectl describe pods/summer-v1-5c4d85c578-sns69 -nprod

GET CURRENT YAML CONFIG FOR DEPLOYMENT:
kubectl get deploy -n prod travel -o yaml --export



kubectl patch deployment travel -nprod -p "{\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"date\":\"`date +'%s'`\"}}}}}"
kubectl patch deployment summer-v1 -nprod -p "{\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"date\":\"`date +'%s'`\"}}}}}"
kubectl patch deployment winter-v1 -nprod -p "{\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"date\":\"`date +'%s'`\"}}}}}"

Wathc deployment update take place:
kubectl get pods -nprod --watch







summer      ClusterIP   10.100.49.207    <none>        9080/TCP   47m
summer-v1   ClusterIP   10.100.169.2     <none>        9080/TCP   95m
travel      ClusterIP   10.100.115.232   <none>        9080/TCP   95m
winter      ClusterIP   10.100.143.136   <none>        9080/TCP   47m
winter-v1   ClusterIP   10.100.217.51    <none>        9080/TCP   95m









kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-namespace.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-deployment.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-service.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-ingress.yaml

kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-namespace.yaml
kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-deployment.yaml
kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-service.yaml
kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.0.0/docs/examples/2048/2048-ingress.yaml


kubectl delete -nprod -f deployment.yaml
kubectl delete -nprod -f service.yaml
kubectl delete -nprod -f ingress.yaml

kubectl apply -nprod -f deployment.yaml
kubectl apply -nprod -f service.yaml
kubectl apply -nprod -f ingress.yaml




kubectl delete -nprod -f deployment.yaml
kubectl apply -nprod -f deployment.yaml






Admin:~/environment/travelapp/MB3/6_Frontend-App (master) $ kubectl get pods -o wide -nprod
NAME                               READY   STATUS    RESTARTS   AGE     IP               NODE                                           NOMINATED NODE   READINESS GATES
2048-deployment-857f497c54-gnnwz   2/2     Running   0          4m19s   192.168.56.38    ip-192-168-45-166.us-west-2.compute.internal   <none>           <none>
2048-deployment-857f497c54-kjw4j   2/2     Running   0          4m19s   192.168.7.60     ip-192-168-31-166.us-west-2.compute.internal   <none>           <none>
2048-deployment-857f497c54-m4bgq   2/2     Running   0          4m19s   192.168.81.215   ip-192-168-79-148.us-west-2.compute.internal   <none>           <none>
2048-deployment-857f497c54-t8rcv   2/2     Running   0          4m19s   192.168.15.40    ip-192-168-5-123.us-west-2.compute.internal    <none>           <none>
2048-deployment-857f497c54-zdmf4   2/2     Running   0          4m19s   192.168.45.14    ip-192-168-45-166.us-west-2.compute.internal   <none>           <none>
summer-v1-784d8b4849-9zmn6         2/2     Running   0          20h     192.168.25.30    ip-192-168-31-166.us-west-2.compute.internal   <none>           <none>
summer-v2-5c9d8958f-gnd2l          2/2     Running   0          20h     192.168.20.150   ip-192-168-31-166.us-west-2.compute.internal   <none>           <none>
travel-55d9df6497-mzmq2            2/2     Running   0          20h     192.168.84.101   ip-192-168-79-148.us-west-2.compute.internal   <none>           <none>
winter-v1-5558b8865f-jhqxx         2/2     Running   0          20h     192.168.75.246   ip-192-168-79-148.us-west-2.compute.internal   <none>           <none>
winter-v2-847bc8cdcf-mf8l9         2/2     Running   0          20h     192.168.18.8     ip-192-168-31-166.us-west-2.compute.internal   <none>           <none>




---------




Bash:

summer = curl summer-v1.prod.svc.cluster.local:9080
winter = curl winter-v1.prod.svc.cluster.local:9080












demoapp-frontend.discoveringthecloud.com
demoapp-app1.discoveringthecloud.com
demoapp-app2.discoveringthecloud.com

480 x 300 image size


codecommit+1-at-841726648246
Ij7/BI8ddBhMz/LGUP4KbSiNxn/w+Hip18PRC2S+Ksk=

      docker tag demoapp-frontend:latest 841726648246.dkr.ecr.us-west-2.amazonaws.com/demoapp-frontend:latest
      docker push 841726648246.dkr.ecr.us-west-2.amazonaws.com/demoapp-frontend:latest


cluster certificate authority data:
aws eks describe-cluster --region us-west-2 --name eksworkshop-eksctl --query cluster.certificateAuthority.data                                               
"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNU1USXhNVEF4TlRjMU1sb1hEVEk1TVRJd09EQXhOVGMxTWxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTFJ2CjRZaXVqeEQ5eHcydTZmZlk0ZmQ2NHdNL2JYUUJjelV3MnZkU2o2ZjM3cFdyWVh5UWkxNWhkTjRZdWZEemdiN3QKZWFNUGxhek5TdzFHY3N6aVVNbnRwODJONVp3Ujd6VmNna0s4aXVKRUdHQmZGS2hOSktVenRwenJrU2NENEFCNgpYSURlaGduMHIrcDRhOGZyMWpPUUhLdjFBWlp1WkQybWZPTDA0UjFoK0lDRW0vd29zVFJQVlUrc25XQ2ozS2JXCjI2YmZSZG5RY2kyeWJJdERmZGpVZnJGcVVyV3lDWEFXTHJNL2JsTlZEOFVuZWY5OGlyekRxN2lqV0RnN0NiNlkKOFA3aTVUTXpQdXBwR29UK2RHQTJtanpORDJHT05xdVpWWm91NFc0b1JqenU2L3FkN09zLzlmMlN0ZFN0WnNqRApXNWRLRjdEdUkxekRaVzNpMFM4Q0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFEcEJRcFVjcUI1MVIvMnI1d1BvanVOTVpzSEQKTmQwanNwN0NVTnRuSW5TUFJXazFKYWI4SWJ3UE50SG9XUUJ2dit0Z3BxR0REZk5OYnR3am14QkFaVTErWjdVVgpRQ3VLS3UvTTdWZlo5VjZveC91aXNTejF3ajZNdXp4SDAvUERIZUNuSEE1a2dpVUUyaGMxSktFblQ2eEpsdEJLClJyOS9PVHBXVDRLR0pENkl2a3NUdVM4WE9xMUJHRlNiamtCU0QrZjFjUzRaTkJkcXN6OTZ5SUQxdDRjN1lPeHkKSHhqUmlnNVZReXFMamsybmNXaG9UTlA0YUQ2ODFkdWFDNFNNbCtjNDM4NjNzamNoK205U0ZDNzEzTXZFdzd5cworNFNpZjFOd096dW9sY2pmL3N0emxNL1FDV2dWbjNqazNzYUU0dFpIV0hzWWdmQWhOTGR1RlJ4SEZNaz0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo="


cluster host address:
aws eks describe-cluster --region us-west-2 --name $CLUSTERNAME --query cluster.endpoint                                                                      
"https://110E030FF2A414188BDA9A73385F65A5.gr7.us-west-2.eks.amazonaws.com"

token:
kubectl get secret default-token-rqjv4 -nprod -o json | jq -r '.data["token"]' | base64 -d                                                                    
eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJwcm9kIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tcnFqdjQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjMwNjAwYmE0LTFiZDYtMTFlYS1hZTJlLTA2OWY5OWYzODJhOCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpwcm9kOmRlZmF1bHQifQ.Imxvg77eMz4j8mxMZd1u1aWdoKzMYwWuQESzYjzx1TxbSZClmlhwVxGb1OyG7v8t43bLl-rAC8CarXzZa0CpZZF4JH1b76B0j7ngo10qppu4928R5JoyUqzi6eynt4xZo7k4l7gbwlceypgUOXzhPMlvq5mO3lHgvgb9Uly2jMh0ISWPqu6HTOULQ6_7kdVZxhCz67rKqODun-Mj9AeUEe9wxqiecDDktTkipecyyXWo6uO8NeIK7_tHarxteRzJPBNqqEGETxt9BDYtPBhvU5hL_eIGA0SkKC1pWRj_loTJhVD9A6w6pgFO2QVSl9D0BUrDI-G4BwJotMF72HOpUA

User:
i-017a224e11cdfccf8@eksworkshop-eksctl.us-west-2.eksctl.io


Clustername:
eksworkshop-eksctl
or
eksworkshop-eksctl.us-west-2.eksctl.io



Admin:~/.kube $ cat config 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNU1USXhNVEF4TlRjMU1sb1hEVEk1TVRJd09EQXhOVGMxTWxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTFJ2CjRZaXVqeEQ5eHcydTZmZlk0ZmQ2NHdNL2JYUUJjelV3MnZkU2o2ZjM3cFdyWVh5UWkxNWhkTjRZdWZEemdiN3QKZWFNUGxhek5TdzFHY3N6aVVNbnRwODJONVp3Ujd6VmNna0s4aXVKRUdHQmZGS2hOSktVenRwenJrU2NENEFCNgpYSURlaGduMHIrcDRhOGZyMWpPUUhLdjFBWlp1WkQybWZPTDA0UjFoK0lDRW0vd29zVFJQVlUrc25XQ2ozS2JXCjI2YmZSZG5RY2kyeWJJdERmZGpVZnJGcVVyV3lDWEFXTHJNL2JsTlZEOFVuZWY5OGlyekRxN2lqV0RnN0NiNlkKOFA3aTVUTXpQdXBwR29UK2RHQTJtanpORDJHT05xdVpWWm91NFc0b1JqenU2L3FkN09zLzlmMlN0ZFN0WnNqRApXNWRLRjdEdUkxekRaVzNpMFM4Q0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFEcEJRcFVjcUI1MVIvMnI1d1BvanVOTVpzSEQKTmQwanNwN0NVTnRuSW5TUFJXazFKYWI4SWJ3UE50SG9XUUJ2dit0Z3BxR0REZk5OYnR3am14QkFaVTErWjdVVgpRQ3VLS3UvTTdWZlo5VjZveC91aXNTejF3ajZNdXp4SDAvUERIZUNuSEE1a2dpVUUyaGMxSktFblQ2eEpsdEJLClJyOS9PVHBXVDRLR0pENkl2a3NUdVM4WE9xMUJHRlNiamtCU0QrZjFjUzRaTkJkcXN6OTZ5SUQxdDRjN1lPeHkKSHhqUmlnNVZReXFMamsybmNXaG9UTlA0YUQ2ODFkdWFDNFNNbCtjNDM4NjNzamNoK205U0ZDNzEzTXZFdzd5cworNFNpZjFOd096dW9sY2pmL3N0emxNL1FDV2dWbjNqazNzYUU0dFpIV0hzWWdmQWhOTGR1RlJ4SEZNaz0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    server: https://110E030FF2A414188BDA9A73385F65A5.gr7.us-west-2.eks.amazonaws.com
  name: eksworkshop-eksctl.us-west-2.eksctl.io
contexts:
- context:
    cluster: eksworkshop-eksctl.us-west-2.eksctl.io
    user: i-017a224e11cdfccf8@eksworkshop-eksctl.us-west-2.eksctl.io
  name: i-017a224e11cdfccf8@eksworkshop-eksctl.us-west-2.eksctl.io
current-context: i-017a224e11cdfccf8@eksworkshop-eksctl.us-west-2.eksctl.io
kind: Config
preferences: {}
users:
- name: i-017a224e11cdfccf8@eksworkshop-eksctl.us-west-2.eksctl.io
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      args:
      - eks
      - get-token
      - --cluster-name
      - eksworkshop-eksctl
      - --region
      - us-west-2
      command: aws
      env: null




sed -i -e "s#\$EKS_CA#$(aws eks describe-cluster --name eksworkshop-eksctl --query cluster.certificateAuthority.data --output text)#g" ./config
sed -i -e "s#\$EKS_CLUSTER_HOST#$(aws eks describe-cluster --name eksworkshop-eksctl --query cluster.endpoint --output text)#g" ./config
sed -i -e "s#\$EKS_CLUSTER_NAME#eksworkshop-eksctl#g" ./config
sed -i -e "s#\$EKS_CLUSTER_USER_NAME#lambda#g" ./config



EKSLambdaTest-role-ao9fsxe1



kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: lambda-access
  namespace: default
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]





kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: lambda-user-role-binding
  namespace: default

subjects:
- kind: User
  name: lambda
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: lambda-access
  apiGroup: rbac.authorization.k8s.io



















import base64
import boto3
import string
import random
from botocore.signers import RequestSigner

class EKSAuth(object):

    METHOD = 'GET'
    EXPIRES = 60
    EKS_HEADER = 'x-k8s-aws-id'
    EKS_PREFIX = 'k8s-aws-v1.'
    STS_URL = 'sts.amazonaws.com'
    STS_ACTION = 'Action=GetCallerIdentity&Version=2011-06-15'

    def __init__(self, cluster_id, region='us-east-1'):
        self.cluster_id = cluster_id
        self.region = region
    
    def get_token(self):
        """
        Return bearer token
        """
        session = boto3.session.Session()
        #Get ServiceID required by class RequestSigner
        client = session.client("sts",region_name=self.region)
        service_id = client.meta.service_model.service_id

        signer = RequestSigner(
            service_id,
            session.region_name,
            'sts',
            'v4',
            session.get_credentials(),
            session.events
        )

        params = {
            'method': self.METHOD,
            'url': 'https://' + self.STS_URL + '/?' + self.STS_ACTION,
            'body': {},
            'headers': {
                self.EKS_HEADER: self.cluster_id
            },
            'context': {}
        }

        signed_url = signer.generate_presigned_url(
            params,
            region_name=session.region_name,
            expires_in=self.EXPIRES,
            operation_name=''
        )

        return (
            self.EKS_PREFIX +
            base64.urlsafe_b64encode(
                signed_url.encode('utf-8')
            ).decode('utf-8')
        )












import os.path
import yaml
import boto3
from kubernetes import client, config
from eksauth import auth
# Configure your cluster name and region here
KUBE_FILEPATH = '/tmp/kubeconfig'
CLUSTER_NAME = 'EKS'
REGION = 'us-east-1'

# We assume that when the Lambda container is reused, a kubeconfig file exists.
# If it does not exist, it creates the file.

if not os.path.exists(KUBE_FILEPATH):
    
    kube_content = dict()
    # Get data from EKS API
    eks_api = boto3.client('eks',region_name=REGION)
    cluster_info = eks_api.describe_cluster(name=CLUSTER_NAME)
    certificate = cluster_info['cluster']['certificateAuthority']['data']
    endpoint = cluster_info['cluster']['endpoint']

    # Generating kubeconfig
    kube_content = dict()
    
    kube_content['apiVersion'] = 'v1'
    kube_content['clusters'] = [
        {
        'cluster':
            {
            'server': endpoint,
            'certificate-authority-data': certificate
            },
        'name':'kubernetes'
                
        }]

    kube_content['contexts'] = [
        {
        'context':
            {
            'cluster':'kubernetes',
            'user':'aws'
            },
        'name':'aws'
        }]

    kube_content['current-context'] = 'aws'
    kube_content['Kind'] = 'config'
    kube_content['users'] = [
    {
    'name':'aws',
    'user':'lambda'
    }]


    # Write kubeconfig
    with open(KUBE_FILEPATH, 'w') as outfile:
        yaml.dump(kube_content, outfile, default_flow_style=False)

def handler(event, context):

    # Get Token
    eks = auth.EKSAuth(CLUSTER_NAME)
    token = eks.get_token()
    # Configure
    config.load_kube_config(KUBE_FILEPATH)
    configuration = client.Configuration()
    configuration.api_key['authorization'] = token
    configuration.api_key_prefix['authorization'] = 'Bearer'
    # API
    api = client.ApiClient(configuration)
    v1 = client.CoreV1Api(api)
    
    # Get all the pods
    ret = v1.list_namespaced_pod("default")

    for i in ret.items:
        print("%s\t%s\t%s" % (i.status.pod_ip, i.metadata.namespace, i.metadata.name))





/lib/python3.6/dist-packages

zip -r9 ~/environment/travelapp/KubernetesLambda.zip main.py
zip -r9 ~/environment/travelapp/KubernetesLambda.zip auth.py

cd $VIRTUAL_ENV/lib/python3.6/site-packages
zip -r9 ~/environment/travelapp/KubernetesLambda.zip .


'user: [ 'lambda' ]

sts.us-west-2.amazonaws.com’.format(self.region)
















vim kubernetes-admin_user
# [...]
users:
- name: kubernetes-admin
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      command: aws-iam-authenticator
      args:
        - "token"
        - "-i"
        - "CLUSTER_ID"
        - "-r"
        - "ROLE_ARN"
  # no client certificate/key needed here!


vim kubernetes-admin_configmap
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
    # statically map arn:aws:iam::000000000000:role/KubernetesAdmin to cluster admin
    - roleARN: arn:aws:iam::841726648246:role/EKSLambdaTest-role-ao9fsxe1
      username: kubernetes-admin
      groups:
      - system:masters
# [... for worker node authentication]



841726648246-aws-eks-codesuite
arn:aws:s3:::841726648246-aws-eks-codesuite
